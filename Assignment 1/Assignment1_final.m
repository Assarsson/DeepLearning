clear;
pkg load image;
function [X, Y, y, N] = LoadBatch(fileName)
  % LoadBatch loads a batch of data as defined in the lab instructions.
  % It reads from that loaded file and puts the data in our X variable
  % our labels in the y variable and then additionally creates a one-hot
  % representation of y and saves it to Y.
  % INPUT:
  %   fileName -- a string containing a .mat-file in the path-environment.
  %
  % OUTPUT:
  %   X -- Matrix of data with size (d, N)
  %   Y -- Matrix of one-hot repesentation with size (K, N)
  %   y -- Vector of ground-truth labels of size (1, N)
  %   N -- Scalar value with value N. For readibility in subs
  % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
  warning('off','all');
  addpath Datasets/cifar-10-batches-mat/;
  inputFile = load(fileName);
  X = im2double(inputFile.data)';
  y = double(inputFile.labels) +1;
  Y = y == 1:max(y);
  N = columns(X);
  Y = Y';
endfunction

function p = Softmax(s)
  % This softmax implementation follow the standard scheme for such a function.
  % It's range is constrained to [0,1] and by summing over the exponents in the
  % denominator, we ensure a 'proper' distribution in a Kolmogorovian sense.
  % The commented line is to increase numerical stability once the gradients are
  % correct.
  % INPUT:
  %   s -- The result of an affine W*X+b-transformation of size (K, N)
  %
  % OUTPUT:
  %   p -- the probability matrix for the classes of X of size (K, N)
  % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
  %s -= max(s);
  p = exp(s) ./ sum(exp(s));
endfunction

function P = EvaluateClassifier(X, W, b)
  % EvaluateClassifier computes the one layer of our forward pass. It is described
  % classicaly by an affine transformation that consists of a linear map and bias term.
  % W*X "twists and turns" the data, whereas the bias term translates it.
  % We then add our preferedd non-linearity, this time defined by the softmax model.
  % As our job is to find 1 of K classes that our image belongs to, our output
  % needs to reflect, in someway, or confidence in the different classes.
  % The softmax function produces a probability distribution over K classes
  % in the sense that each value is confined in [0,1] and sum(P) == 1. As it contains
  % exponentiations and normalizations, we conclude that our affine transformation
  % produces un-normalized (normalized by the division) log-probabilities (converted with exp).
  % This also explains the choice of cross-entropy loss, as that loss function minimizes
  % distributional distances between our guessed distribution from this function, and our
  % true labels in Y or y.
  % INPUT:
  %   X -- The current data batch of size (d, N_batch)
  %   W -- The current weight matrix of size (K, d)
  %   b -- The current bias vector of size (K, 1)
  %
  % OUTPUT:
  %   P -- the probability matrix for the classes of X of size (K, N)
  % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
  s = W*X + b;
  %P = Softmax(s);
  P = exp(s)./ sum(exp(s));
endfunction

function J = ComputeCost(X, Y, W, b, N, lambda)
  % ComputeCost computes the total cost of generating our guessed distribution
  % from the correct distribution of labels, with a cross-entropy loss and
  % and a L2-regularization-term. As the Crossentropy between P and y can be
  % expressed in terms of the Kullback-Leibler-divergence ('KL') between them
  % it can be viewed as calculating the analogous "distance" between
  % the two distributions. (Albeit it is not a true Lp-distance.)
  % Probabilistically, we can view it as that the minimization of that "distance"
  % is the same as maximizing the probability of the data, given our parameters.
  % This is the same as a MLE-estimate, or a MAP-estimate if we assume that the
  % regularization term acts as a Gaussian prior on our parameter space.
  % INPUT:
  %   X -- The current data batch of size (d, N_batch)
  %   Y -- The current one-hot label representation of size (K, N_batch)
  %   W -- The current weight matrix of size (K, d)
  %   b -- The current bias vector of size (K, 1)
  %   N -- The current batch size, i.e. N == N_batch
  %   lambda -- The current scalar regularization parameter
  %
  % OUTPUT:
  %   J -- The scalar Cost-value.
  % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
  J = 0;
  P = EvaluateClassifier(X, W, b);
  J = -sum(log(sum(Y.*P)))/N + lambda*sum(sumsq(W));
endfunction

function acc = ComputeAccuracy(X, y, W, b, N)
  % ComputeAccuracy computes the proportion of correct guessed classes
  % of total classes tried. It sums the list of instances where y - facit
  % is equal to our guessed class, extracted as the max prob-value from our
  % distribution vector P, generated by evaluating the current parameters W and b.
  %
  % INPUT:
  %   X -- The current data batch of size (d, N_batch)
  %   y -- The current label vector of size (1, N_batch)
  %   W -- The current weight matrix of size (K, d)
  %   b -- The current bias vector of size (K, 1)
  %   N -- The current batch size, i.e. N == N_batch
  %
  % OUTPUT:
  %   acc -- The scalar accuracy as calculated by the fraction of correct/total.
  % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

  P = EvaluateClassifier(X, W, b);
  [_,PMaxIdx] = max(P);
  acc = sum(PMaxIdx' == y);
  acc = acc/N;
endfunction

function [grad_W, grad_b] = ComputeGradients(X, Y, P, W, N, lambda)
  % ComputeGradients computes the gradients of W and b as defined by differentiating
  % the cost function with respects to the node in the computational graph and traversing it
  % likeso, until we reach the parameter variable of interest. As we have a 1-layer shallow network
  % this results in us only needing to update DL/DW and DL/Db. The gradient of b is defined solely
  % by the gradient-function g(), stemming from the cross-entropy loss function. W has the
  % inner derivative from multiplication with X, as well as the derivative of the reguralization
  % term.
  % INPUT:
  %   X -- The current data batch of size (d, N_batch)
  %   Y -- The current one-hot label representation of size (K, N_batch) (true distribution)
  %   P -- The current probability distribution calculated by our parametrized model.
  %   W -- The current weight matrix of size (K, d)
  %   N -- The current batch size, i.e. N == N_batch
  %   lambda -- The current scalar regularization parameter
  %
  % OUTPUT:
  %   grad_W -- The gradient w.r.t W of size (K, d)
  %   grad_b -- The gradient w.r.t b of size (K, 1)
  % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
  grad_W = zeros(size(W));
  grad_b = zeros(rows(W), 1);
  for i=1:N
    Xi = X(:,i);
    Yi = Y(:,i);
    Pi = P(:,i);
    g = -Yi'/(Yi'*Pi)*(diag(Pi)-Pi*Pi'); % We missed a f****** minus sign.
    grad_b = grad_b + g';
    grad_W = grad_W + g'*Xi';
  end

  regularization_term = 2*lambda*W;
  grad_b = grad_b/N;
  grad_W = grad_W/N;
  grad_W = grad_W + regularization_term;

endfunction

function [grad_b, grad_W] = ComputeGradsNumSlow(X, Y, W, b, N, lambda, h)

no = size(W, 1);
d = size(X, 1);

grad_W = zeros(size(W));
grad_b = zeros(no, 1);

for i=1:length(b)
    b_try = b;
    b_try(i) = b_try(i) - h;
    c1 = ComputeCost(X, Y, W, b_try, N, lambda);
    b_try = b;
    b_try(i) = b_try(i) + h;
    c2 = ComputeCost(X, Y, W, b_try, N, lambda);
    grad_b(i) = (c2-c1) / (2*h);
end

for i=1:numel(W)

    W_try = W;
    W_try(i) = W_try(i) - h;
    c1 = ComputeCost(X, Y, W_try, b, N, lambda);

    W_try = W;
    W_try(i) = W_try(i) + h;
    c2 = ComputeCost(X, Y, W_try, b, N, lambda);

    grad_W(i) = (c2-c1) / (2*h);
end
endfunction

function maxDiff = GradChecker(grad_analytic, grad_numeric)
  % Self implemented gradient checker. It follows the instruction from the lab
  % specification and computes the relative error between our gradient and the
  % numerically generated one.
  % INPUT:
  %   grad_analytic -- our calculated gradient of size (K, d)
  %   grad_numeric  -- Some numerically calculated gradient approximation (K, d)
  %
  % OUTPUT:
  %   maxDiff -- the largest recorded element-wise difference between our gradients.
  % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

  if (size(grad_analytic) != size(grad_numeric))
    maxDiff = 0;
    disp('Please input gradients with corresponding dimensions');
    return;
  endif
  difference = norm(grad_analytic - grad_numeric)./max(1e-6, norm(grad_analytic)+norm(grad_numeric));
  maxDiff = max(max(difference));
  return;
endfunction

function [Wstar, bstar] = MiniBatchGD(X, Y,eta, W, b, N,lambda)
  % MiniBatchGD performs the entire forward and backward pass.
  % It divides the dataset (X, Y)-pairs into batches and computes
  % the forward pass to generate a batch-determined class probability distribution
  % P. It then computes the backwardpass gradients and updates the W and b
  % elements, scaled by our learning-rate eta. As W and b isn't dependent on
  % the number of training-examples (N) (check their shapes), they are always
  % full.
  % INPUT:
  %   X -- The current data batch of size (d, N)
  %   Y -- The current one-hot label matrix of size (K, N)
  %   y -- The current label matrix of size (1, N)
  %   GDparams -- Object containing scalars for batchsize, eta and epochs.
  %   W -- The current weight matrix of size (K, d)
  %   b -- The current bias vector of size (K, 1)
  %   N -- The scalar value representing number of examples.
  %   lambda -- The weighting parameter of our regularization term.
  %
  % OUTPUT:
  %   Wstar -- The final computed gradient for W of size (K, d)
  %   bstar -- The final computed gradient for b of size (K, 1)
  % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
  P = EvaluateClassifier(X, W, b);
  [grad_W, grad_b] = ComputeGradients(X, Y, P, W, N,lambda);
  Wstar = W - eta*grad_W;
  bstar = b - eta*grad_b;
endfunction

function [W, b] = Initialize(K, d, initType)
  % Initialize creates our W and b matrices and populates them with random values.
  % It either utilizes a random gaussian prior on the parameters or a Xavier prior.
  % The role of the Xavier prior is to keep X and and W*X equivariant, to increase
  % stability of the model and increase speed in our iterations.
  % INPUT:
  %   K -- The number of classes for our problem
  %   d -- The dimensionality of our input X.
  %   initType -- Optional variable that can invoke xavier-initialization.
  %
  % OUTPUT:
  %   W -- A populated weight matrix of size (K, d)
  %   b -- A populated bias vector of size (K, 1)
  % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
  W = double(randn(K,d));
  b = double(randn(K,1));
  mu = 0;
  if nargin < 3
    variance = 0.001;
  elseif (initType == 'xavier')
    variance = 1/d;
  elseif (initType == 'norand')
    variance = 0.01;
    W = double(ones(K,d));
    b = double(ones(K,1));
  end
  W = W.*sqrt(variance) + mu;
  b = b.*sqrt(variance) + mu;
  return;
endfunction

[Xtrain, Ytrain, ytrain, Ntrain] = LoadBatch('data_batch_1.mat');
[Xval, Yval, yval, Nval] = LoadBatch('data_batch_2.mat');
[Xtest, Ytest, ytest, Ntest] = LoadBatch('test_batch.mat');
K = max(ytrain);
d = rows(Xtrain);
N = Ntrain;
lambda = 1;
n_batch = 100;
eta = 0.01;
n_epochs = 40;
%W = double(0.01.*randn(K,d));
%b = double(0.01.*randn(K,1));
[W, b] = Initialize(K,d);
J_train = zeros(n_epochs, 1);
J_val = zeros(n_epochs, 1);
P = EvaluateClassifier(Xtrain, W, b);
[dW_a, db_a] = ComputeGradients(Xtrain, Ytrain, P, W, N, lambda);
[db_n, dW_n] = ComputeGradsNumSlow(Xtrain, Ytrain, W, b, N, lambda, 1e-6);
W_diff = GradChecker(dW_a, dW_n);
disp(W_diff);
b_diff = GradChecker(db_a, db_n);
disp(b_diff);
for i = 1:n_epochs
  for j = 1:N/n_batch
    j_start = (j-1)*n_batch + 1;
    j_end = j*n_batch;
    inds = j_start:j_end;
    Xbatch = Xtrain(:,inds);
    Ybatch = Ytrain(:,inds);
    [W,b] = MiniBatchGD(Xbatch, Ybatch, eta, W, b, n_batch,lambda);
  endfor
  costTrain = ComputeCost(Xtrain, Ytrain, W, b, N, lambda);
  accTrain = ComputeAccuracy(Xtrain, ytrain, W, b, N);
  disp('Cost at current epoch: '),disp(costTrain);
  disp('Accuracy at current epoch: '),disp(accTrain);
  J_train(i) = costTrain;
  J_val(i) = ComputeCost(Xval, Yval, W, b, N, lambda);
endfor

graphics_toolkit gnuplot;
fig = figure();
set(fig, 'visible', 'off');
set(0, 'defaultaxesfontname', 'Helvetica');
hold on;
plot(1:n_epochs, J_train, 'b', 1:n_epochs, J_val, 'y');
title('Cost with lambda = 1, #epochs = 40, #batches = 100, eta = 0.01');
legend('Training Cost', 'Validation Cost');
print -djpg image.jpg;
accuracy = ComputeAccuracy(Xtest, ytest, W, b, N);
disp(accuracy);
mt = [];
for i = 1:K
  im = reshape(W(i, :), 32, 32, 3);
  s_im{i} = (im - min(im(:)))/(max(im(:)) - min(im(:)));
  s_im{i} = permute(s_im{i}, [2, 1, 3]);
  mt = [mt s_im{i}];
end
montage(s_im, 'size', [1,K]);

pause(50);
