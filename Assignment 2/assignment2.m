addpath Datasets/cifar-10-batches-mat/;

%Here lies the appended functions for the hand-in
function acc = ComputeAccuracy(X, y, W, b, N)
  % ComputeAccuracy computes the proportion of correct guessed classes
  % of total classes tried. It sums the list of instances where y - facit
  % is equal to our guessed class, extracted as the max prob-value from our
  % distribution vector P, generated by evaluating the current parameters W and b.
  %
  % INPUT:
  %   X -- The current data batch of size (d, N_batch)
  %   y -- The current label vector of size (1, N_batch)
  %   W -- The current weight cell of size (2,1) containing W1 and W2
  %   b -- The current bias cell of size (2, 1) containing b1 and b2
  %   N -- The current batch size, i.e. N == N_batch
  %
  % OUTPUT:
  %   acc -- The scalar accuracy as calculated by the fraction of correct/total.
  % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

  cache = EvaluateClassifier(X, W, b);
  [_,PMaxIdx] = max(cache{4,1});
  acc = sum(y' == PMaxIdx);
  acc = acc/N;
endfunction

function J = ComputeCost(X, Y, W, b, N, lambda)
  % ComputeCost computes the total cost of generating our guessed distribution
  % from the correct distribution of labels, with a cross-entropy loss and
  % and a L2-regularization-term. As the Crossentropy between P and y can be
  % expressed in terms of the Kullback-Leibler-divergence ('KL') between them
  % it can be viewed as calculating the analogous "distance" between
  % the two distributions. (Albeit it is not a true Lp-distance.)
  % Probabilistically, we can view it as that the minimization of that "distance"
  % is the same as maximizing the probability of the data, given our parameters.
  % This is the same as a MLE-estimate, or a MAP-estimate if we assume that the
  % regularization term acts as a Gaussian prior on our parameter space.
  % INPUT:
  %   X -- The current data batch of size (d, N_batch)
  %   Y -- The current one-hot label representation of size (K, N_batch)
  %   W -- The current weight cell of size (2,1) containing W1 and W2
  %   b -- The current bias cell of size (2,1) containing b1 and b2
  %   N -- The current batch size, i.e. N == N_batch
  %   lambda -- The current scalar regularization parameter
  %
  % OUTPUT:
  %   J -- The scalar Cost-value.
  % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
  J = 0;
  W1 = W{1, 1};
  W2 = W{2, 1};
  cache = EvaluateClassifier(X, W, b);
  P = cache{4, 1};
  J = -sum(log(sum(Y.*P)))/N + lambda*(sum(sumsq(W1)) + sum(sumsq(W2)));
endfunction

function [grad_b,grad_W] = ComputeGradients(X, Y, cache, W, b, N, lambda)
  % ComputeGradients computes the gradients of W and b as defined by differentiating
  % the cost function with respects to the node in the computational graph and traversing it
  % likeso, until we reach the parameter variable of interest. As we have a 2-layer network
  % this results in us needing to update DL/DW1, DL/DW2, DL/Db1 and DL/Db2.
  % INPUT:
  %   X -- The current data batch of size (d, N_batch)
  %   Y -- The current one-hot label representation of size (K, N_batch) (true distribution)
  %   P -- The current probability distribution calculated by our parametrized model.
  %   W -- The current weight cell of size (2, 1) containing W1 and W2
  %   b -- The current bias cell of size (2,1) containing b1 and b2
  %   N -- The current batch size, i.e. N == N_batch
  %   lambda -- The current scalar regularization parameter
  %
  % OUTPUT:
  %   grad_W -- The gradient w.r.t W of size (K, d)
  %   grad_b -- The gradient w.r.t b of size (K, 1)
  % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
  grad_W = cell(2,1);
  grad_b = cell(2,1);
  grad_W1 = zeros(size(W{1,1}));
  grad_b1 = zeros(size(b{1,1}));
  grad_W2 = zeros(size(W{2,1}));
  grad_b2 = zeros(size(b{2,1}));
  h = cache{2,1};
  W2 = W{2,1};
  W1 = W{1,1};
  s1 = cache{1,1};
  P = cache{4,1};
  for i=1:N
    Xi = X(:,i);
    Yi = Y(:,i);
    Pi = P(:,i);
    hi = h(:,i);
    s1i = s1(:,i);
    g = -Yi'*(diag(Pi) - Pi*Pi')/(Yi'*Pi); % We missed a f****** minus sign.
    grad_b2 += g';
    grad_W2 += g'*hi';
    g = g*W2;
    g = g*diag(s1i > 0);
    grad_b1 += g';
    grad_W1 += g'*Xi';
  end

  grad_b1 /= N;
  grad_W1 /= N;
  grad_b2 /= N;
  grad_W2 /= N;
  grad_W1 += 2*lambda*W1;
  grad_W2 += 2*lambda*W2;
  grad_W(1,1) = grad_W1;
  grad_b(1,1) = grad_b1;
  grad_W(2,1) = grad_W2;
  grad_b(2,1) = grad_b2;
endfunction

function cache = EvaluateClassifier(X, W, b)
  % EvaluateClassifier computes the two layers of our forward pass. It is described
  % classicaly by an affine transformation that consists of a linear map and bias term.
  % W*X "twists and turns" the data, whereas the bias term translates it.
  % We then add our preferedd non-linearity, this time defined by the Relu model.
  % The relu model has non-saturating gradients and helps with disentanglement, representation
  % linear separability and "bagging". See 'Deep Sparse Rectifier Neural Networks' for more
  % information.
  % As our job is to find 1 of K classes that our image belongs to, our output
  % needs to reflect, in someway, or confidence in the different classes.
  % The softmax function produces a probability distribution over K classes
  % in the sense that each value is confined in [0,1] and sum(P) == 1. As it contains
  % exponentiations and normalizations, we conclude that our affine transformation
  % produces un-normalized (normalized by the division) log-probabilities (converted with exp).
  % This also explains the choice of cross-entropy loss, as that loss function minimizes
  % distributional distances between our guessed distribution from this function, and our
  % true labels in Y or y.
  % INPUT:
  %   X -- The current data batch of size (d, N_batch)
  %   W -- The current weight cell of size (2, 1) containing W1 and W2
  %   b -- The current bias cell of size (2, 1) containing b1 and b2
  %
  % OUTPUT:
  %   P -- the probability matrix for the classes of X of size (K, N)
  % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
  cache = cell(4,1);
  W1 = W{1, 1};
  b1 = b{1, 1};
  W2 = W{2, 1};
  b2 = b{2, 1};
  s1 = W1*X + b1;
  h = Relu(s1);
  s = W2*h + b2;
  P = Softmax(s);
  cache(1,1) = s1;
  cache(2,1) = h;
  cache(3,1) = s;
  cache(4,1) = P;
endfunction

function params = Generateparams(min_value, max_value, no_params)
  % Generateparams uniformly at random samples on the log scale given by the input.
  % it generates a variable amount of results and returns a vector containing these.
  %   min_value -- the lower log-bound of sampling
  %   max_value -- the upper log-bound of sampling
  %   no_params -- The number of parameters to sample
  %
  % OUTPUT:
  %   params -- A populated parameter vector of size (no_params, 1)
  % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
params = 10.^(min_value+(max_value-min_value)*rand(no_params,1)');
endfunction

function maxDiff = GradChecker(grad_analytic, grad_numeric)
  % Self implemented gradient checker. It follows the instruction from the lab
  % specification and computes the relative error between our gradient and the
  % numerically generated one.
  % INPUT:
  %   grad_analytic -- our calculated gradient of size (K, d)
  %   grad_numeric  -- Some numerically calculated gradient approximation (K, d)
  %
  % OUTPUT:
  %   maxDiff -- the largest recorded element-wise difference between our gradients.
  % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
  difference = norm(grad_analytic - grad_numeric)./max(1e-6, norm(grad_analytic)+norm(grad_numeric));
  maxDiff = max(max(difference));
  return;
endfunction

function [W, b] = Initialize(K, d, hiddenNodes, initType)
  % Initialize creates our W and b cells and populates them with random values.
  % It either utilizes a random gaussian prior on the parameters or a Xavier prior.
  % The role of the Xavier prior is to keep X and and W*X equivariant, to increase
  % stability of the model and increase speed in our iterations.
  % INPUT:
  %   K -- The number of classes for our problem
  %   d -- The dimensionality of our input X.
  %   initType -- Optional variable that can invoke xavier-initialization.
  %
  % OUTPUT:
  %   W -- A populated weight matrix of size (K, d)
  %   b -- A populated bias vector of size (K, 1)
  % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
  W = cell(2,1);
  b = cell(2,1);
  W1 = double(randn(hiddenNodes,d));
  W2 = double(randn(K, hiddenNodes));
  b1 = double(zeros(hiddenNodes,1));
  b2 = double(zeros(K,1));
  mu = 0;
  variance1 = 0.000001;
  variance2 = 0.000001;
  if nargin < 4
    variance1 = 0.000001;
    variance2 = 0.000001;
  elseif (initType == 'xavier')
    variance1 = 2/d;
    variance2 = 2/hiddenNodes;
  elseif (initType == 'norand')
    variance = 0.1;
    W = double(ones(K,d));
    b = double(ones(K,1));
  end
  W1 = W1*sqrt(variance1) + mu;
  W2 = W2*sqrt(variance2) + mu;
  W(1,1) = W1;
  b(1,1) = b1;
  W(2,1) = W2;
  b(2,1) = b2;

endfunction

function [Wm, bm] = InitializeMomentum(W, b)
  % Initialize creates our Wm and bm cells and populates them with zeros.
  % INPUT:
  %   W -- the current weight cell of size (2,1) containing W1 and W2
  %   b -- the current bias cell of size (2,1) containing b1 and b2
  %
  % OUTPUT:
  %   Wm -- A populated weight momentum cell of size (2, 1)
  %   bm -- A populated bias momentum cell of size (2, 1)
  % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
  Wm = cell(2,1);
  bm = cell(2,1);
  Wm(1,1) = zeros(size(W{1,1}));
  Wm(2,1) = zeros(size(W{2,1}));
  bm(1,1) = zeros(size(b{1,1}));
  bm(2,1) = zeros(size(b{2,1}));

endfunction

function [X, Y, y, N] = LoadBatch(fileName)
  % LoadBatch loads a batch of data as defined in the lab instructions.
  % It reads from that loaded file and puts the data in our X variable
  % our labels in the y variable and then additionally creates a one-hot
  % representation of y and saves it to Y.
  % INPUT:
  %   fileName -- a string containing a .mat-file in the path-environment.
  %
  % OUTPUT:
  %   X -- Matrix of data with size (d, N)
  %   Y -- Matrix of one-hot repesentation with size (K, N)
  %   y -- Vector of ground-truth labels of size (1, N)
  %   N -- Scalar value with value N. For readibility in subs
  % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
  warning('off','all');
  addpath Datasets/cifar-10-batches-mat/;
  inputFile = load(fileName);
  X = im2double(inputFile.data)';
  y = double(inputFile.labels) +1;
  Y = y == 1:max(y);
  N = columns(X);
  Y = Y';
endfunction

function [Wstar, bstar, Wm, bm] = MiniBatchGD(Xbatch, Ybatch,eta, W, b, Wm, bm, N,lambda, rho)
  % MiniBatchGD performs the entire forward and backward pass.
  % It divides the dataset (X, Y)-pairs into batches and computes
  % the forward pass to generate a batch-determined class probability distribution
  % P. It then computes the backwardpass gradients and updates the W and b
  % elements, scaled by our learning-rate eta. As W and b isn't dependent on
  % the number of training-examples (N) (check their shapes), they are always
  % full.
  % INPUT:
  %   X -- The current data batch of size (d, N)
  %   Y -- The current one-hot label matrix of size (K, N)
  %   eta -- the current learning rate
  %   W -- The current weight cell of size (2, 1) containing W1 and W2
  %   b -- The current bias cell of size (2, 1) containing b1 and b2
  %   Wm -- The current weight momentum cell of size (2, 1) containing Wm1 and Wm2
  %   bm -- The current bias momentum cell of size (2, 1) containing bm1 and bm2
  %   N -- The scalar value representing number of examples.
  %   lambda -- The weighting parameter of our regularization term.
  %   rho -- The momentum term, a scalar betwenn 0 and 1
  %
  % OUTPUT:
  %   Wstar -- The final computed gradient for W of size (K, d)
  %   bstar -- The final computed gradient for b of size (K, 1)
  %   Wm -- The updated weight momentum cell of size (2, 1) containing Wm1 and Wm2
  %   bm -- The updated bias momentum cell of size (2, 1) containing bm1 and bm2
  % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
  Wstar = cell(2,1);
  bstar = cell(2,1);
  cache = EvaluateClassifier(Xbatch, W, b);
  [grad_b, grad_W] = ComputeGradients(Xbatch, Ybatch, cache, W,b, N,lambda);
  Wm(1,1) = rho*Wm{1,1} + eta*grad_W{1,1};
  Wstar(1,1) = W{1,1}-Wm{1,1};
  %Wstar(1,1) = W{1,1}-eta*grad_W{1,1};

  Wm(2,1) = rho*Wm{2,1} + eta*grad_W{2,1};
  Wstar(2,1) = W{2,1} - Wm{2,1};
  %Wstar(2,1) = W{2,1}-eta*grad_W{2,1};

  bm(1,1) = rho*bm{1,1} + eta*grad_b{1,1};
  bstar(1,1) = b{1,1}-bm{1,1};
  %bstar(1,1) = b{1,1}-eta*grad_b{1,1};

  bm(2,1) = rho*bm{2,1} + eta*grad_b{2,1};
  bstar(2,1) = b{2,1} - bm{2,1};
  %bstar(2,1) = b{2,1}-eta*grad_b{2,1};
endfunction

function [Xstar, mean_of_X] = Preprocess(X)
  % Preprocess normalizes the dataset by subtracting the mean.
  % This will give the resulting data a zero mean across the examples.
  % INPUT:
  %   X -- An un-normalized data matrix of size (d,N).
  %
  % OUTPUT:
  %   Xstar -- a normalized data matrix of size (d,N)
  %   mean_of_x -- a mean-vector of size (d)
  % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
  mean_of_X = mean(X, 2);
  Xstar = X - repmat(mean_of_X, [1, size(X, 2)]);
endfunction

function h = Relu(s)
  % This activation function is the relu-activation function.
  % INPUT:
  %   s -- The result of an affine W*X+b-transformation of size (m, N)
  %
  % OUTPUT:
  %   h --  non-linearity output of size (m, N)
  % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
  h = max(0, s);
endfunction

function p = Softmax(s)
  % This softmax implementation follow the standard scheme for such a function.
  % It's range is constrained to [0,1] and by summing over the exponents in the
  % denominator, we ensure a 'proper' distribution in a Kolmogorovian sense.
  % The commented line is to increase numerical stability once the gradients are
  % correct.
  % INPUT:
  %   s -- The result of an affine W*X+b-transformation of size (K, N)
  %
  % OUTPUT:
  %   p -- the probability matrix for the classes of X of size (K, N)
  % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
  %s -= max(s);
  p = exp(s) ./ sum(exp(s));
endfunction

function [grad_b, grad_W] = ComputeGradsNumSlow(X, Y, W, b, N, lambda, h)

grad_W = cell(numel(W), 1);
grad_b = cell(numel(b), 1);

for j=1:length(b)
    grad_b{j} = zeros(size(b{j}));

    for i=1:length(b{j})

        b_try = b;
        b_try{j}(i) = b_try{j}(i) - h;
        c1 = ComputeCost(X, Y, W, b_try, N, lambda);

        b_try = b;
        b_try{j}(i) = b_try{j}(i) + h;
        c2 = ComputeCost(X, Y, W, b_try, N, lambda);

        grad_b{j}(i) = (c2-c1) / (2*h);
    end
end

for j=1:length(W)
    grad_W{j} = zeros(size(W{j}));

    for i=1:numel(W{j})

        W_try = W;
        W_try{j}(i) = W_try{j}(i) - h;
        c1 = ComputeCost(X, Y, W_try, b, N, lambda);

        W_try = W;
        W_try{j}(i) = W_try{j}(i) + h;
        c2 = ComputeCost(X, Y, W_try, b, N, lambda);

        grad_W{j}(i) = (c2-c1) / (2*h);
    end
end
endfunction


function [X, Y, y, N] = LoadAll(fileList)
  X = [];
  Y = [];
  y = [];
  for i=1:length(fileList)
  [Xi, Yi, yi, N] = LoadBatch(fileList{i});
  X = [X Xi];
  Y = [Y Yi];
  y = [y yi'];
  endfor
  N = columns(X);
endfunction

%Here ends the appending of functions for the hand-in



[Xtrain, Ytrain, ytrain, Ntrain] = LoadAll({'data_batch_1.mat'});
[Xval, Yval, yval, Nval] = LoadBatch('data_batch_5.mat');
[Xtest, Ytest, ytest, ntest] = LoadBatch('test_batch.mat');
ytrain = ytrain';
d = rows(Xtrain);
N = Ntrain;
K = rows(Ytrain);
Xtrain = Xtrain(:,1:N);
Ytrain = Ytrain(:,1:N);
ytrain = ytrain(1:N,:);
Ntrain = N;
hiddenNodes = 50;
[Xtrain, mean_of_Xtrain] = Preprocess(Xtrain);
n_epochs = 10;
n_batch = 64;
rho = 0.999;
no_etas = 2;
no_lambdas = 10;
etas = Generateparams(-1.70,-1.52,no_etas);
lambdas = Generateparams(-4.7,-2.60,no_lambdas);
titleText = ['searching over a total of ' num2str(no_etas*no_lambdas) ' parameters.'];
disp(titleText);
lambdas = [1e-6];
etas = [0.01, 0.1, 1];

%%%% Gradient checking procedure
%lambda = 0;
%[W, b] = Initialize(K, d, hiddenNodes);
%cache = EvaluateClassifier(Xtrain, W, b);
%[grad_b_a, grad_W_a] = ComputeGradients(Xtrain, Ytrain, cache, W, b, N, lambda);
%[grad_b_n, grad_W_n] = ComputeGradsNumSlow(Xtrain, Ytrain, W, b, N, 0, 1e-5);
%for i=1:2
%  disp(GradChecker(grad_W_a{i}, grad_W_n{i}));
%  disp(GradChecker(grad_b_a{i}, grad_b_n{i}));
%endfor
bestAccuracies = [];
for lambda = lambdas
  for eta = etas
    [W, b] = Initialize(K, d, hiddenNodes, 'xavier');
    [Wm, bm] = InitializeMomentum(W,b);
    J_train = [];
    J_val = [];
    tempAccuracies = [];
    tic;
    iterator = 0;
    for i=1:n_epochs
      iterator += 1;
      for j=1:N/n_batch
        j_start = (j-1)*n_batch + 1;
        j_end = j*n_batch;
        inds = j_start:j_end;
        Xbatch = Xtrain(:,inds);
        Ybatch = Ytrain(:,inds);
        [W,b,Wm,bm] = MiniBatchGD(Xbatch, Ybatch, eta, W, b, Wm, bm, n_batch,lambda, rho);
      endfor
      costTrain = ComputeCost(Xtrain, Ytrain, W, b, N, lambda);
      accTrain = ComputeAccuracy(Xtrain, ytrain, W, b, N);
      disp('cost: '),disp(costTrain);
      disp('acc: '),disp(accTrain);
      J_train = [J_train costTrain];
      costVal = ComputeCost(Xval-repmat(mean_of_Xtrain,[1,size(Xval,2)]), Yval, W, b, Nval, lambda);
      J_val = [J_val costVal];
      disp('current validation cost: '),disp(costVal);
    endfor
    disp('Time for evaluating one parameter setting: ');
    toc;
    disp(ComputeAccuracy(Xtest-repmat(mean_of_Xtrain,[1,size(Xtest,2)]),ytest, W, b,ntest));
    tempAccuracies = [tempAccuracies ComputeAccuracy(Xtest-repmat(mean_of_Xtrain,[1,size(Xtest,2)]),ytest, W, b,ntest)];
    bestAccuracies = [bestAccuracies ['accuracy: ' num2str(max(tempAccuracies)) ' for lambda ' num2str(lambda) ' and eta ' num2str(eta) char(10)]];
    graphics_toolkit gnuplot;
    fig = figure();
    set(fig, 'visible', 'off');
    set(0, 'defaultaxesfontname', 'Helvetica');
    hold on;
    titleText = ['Cost with lambda = ' num2str(lambda) ' #epochs = ' num2str(n_epochs) ' #batches = ' num2str(n_batch) ' eta = ' num2str(eta) ' examples = ' num2str(N)];
    plot(1:iterator, J_train, 'b', 1:iterator, J_val, 'y');
    title(titleText);
    imageName = ['eta' num2str(eta) '+lambda' num2str(lambda) '.jpg'];
    legend('Training Cost', 'Validation Cost');
    print(imageName);
  endfor
endfor
disp(ComputeAccuracy(Xval-repmat(mean_of_Xtrain,[1,size(Xval,2)]), yval, W, b, Nval));
disp(bestAccuracies);
save bestAccuracies.txt bestAccuracies;
disp('finished!');
