addpath Datasets/cifar-10-batches-mat/
function s = BatchNormalize(s, mu, v)
%
%
% INPUT:
%   s  -- the output from our linear transformation at each layer
%   mu -- estimated mean for the unnormalized s-value
%   v  -- vector of estimated variance for each dimension of s
%
% OUTPUT:
%   s  -- the batch normalized version of input-s.
%
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
epsilon = 1e-5;
s = diag(v+epsilon)^(-0.5)*(s-mu);
endfunction

function ng = BatchNormBackPass(g, s, mu, v, batchSize)
  ng = cell(batchSize, 1);
  epsilon = 1e-5;
  Vb = diag(v + epsilon);
  s = num2cell(s, 1);
  grad_v = cellfun(@(x, y) (y*Vb^(-1.5)*diag(x-mu))',s',g,'UniformOutput', false);
  grad_v = -0.5*sum([grad_v{:}], 2)';
  grad_mu = -sum([cellfun(@(x) (x*Vb^(-0.5))', g, 'UniformOutput', false){:}], 2);
  ng = cellfun(@(x, y) y*Vb^(-0.5)+2/batchSize*grad_v*diag(x-mu) + grad_mu'/batchSize, s',g, 'UniformOutput', false);
endfunction

function acc = ComputeAccuracy(X, y, W, b, N, mav, vav)
  % ComputeAccuracy computes the proportion of correct guessed classes
  % of total classes tried. It sums the list of instances where y - facit
  % is equal to our guessed class, extracted as the max prob-value from our
  % distribution vector P, generated by evaluating the current parameters W and b.
  %
  % INPUT:
  %   X -- The current data batch of size (d, N_batch)
  %   y -- The current label vector of size (1, N_batch)
  %   W -- The current weight cell of size (2,1) containing W1 and W2
  %   b -- The current bias cell of size (2, 1) containing b1 and b2
  %   N -- The current batch size, i.e. N == N_batch
  %
  % OUTPUT:
  %   acc -- The scalar accuracy as calculated by the fraction of correct/total.
  % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
  layers = length(W);
  [P, S, Shat, H, mus, vs] = EvaluateClassifier(X, W, b, mav, vav);
  [_,PMaxIdx] = max(P);
  acc = sum(y' == PMaxIdx)/N;
endfunction

function J = ComputeCost(X, Y, W, b, N, lambda, varargin)
  % ComputeCost computes the total cost of generating our guessed distribution
  % from the correct distribution of labels, with a cross-entropy loss and
  % and a L2-regularization-term. As the Crossentropy between P and y can be
  % expressed in terms of the Kullback-Leibler-divergence ('KL') between them
  % it can be viewed as calculating the analogous "distance" between
  % the two distributions. (Albeit it is not a true Lp-distance.)
  % Probabilistically, we can view it as that the minimization of that "distance"
  % is the same as maximizing the probability of the data, given our parameters.
  % This is the same as a MLE-estimate, or a MAP-estimate if we assume that the
  % regularization term acts as a Gaussian prior on our parameter space.
  % INPUT:
  %   X -- The current data batch of size (d, N_batch)
  %   Y -- The current one-hot label representation of size (K, N_batch)
  %   W -- The current weight cell of size (2,1) containing W1 and W2
  %   b -- The current bias cell of size (2,1) containing b1 and b2
  %   N -- The current batch size, i.e. N == N_batch
  %   lambda -- The current scalar regularization parameter
  %
  % OUTPUT:
  %   J -- The scalar Cost-value.
  % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
  J = 0;
  layers = length(W);
  if (length(varargin) == 0)
    [P, S, Shat, H, mus, vs] = EvaluateClassifier(X, W, b);
  else
    [P, S, Shat, H, mus, vs] = EvaluateClassifier(X, W, b, varargin{1,1}, varargin{1,2});
  endif
  weightCost = cellfun(@(x) sum(sumsq(x)), W, 'UniformOutput', true);
  regCost = lambda*sum(weightCost);
  J = -sum(log(sum(Y.*P)))/N + regCost;
endfunction

function [grad_b,grad_W] = ComputeGradients(X, Y, P, S, Shat, H, mus, vs, W, b, N, lambda)
  % ComputeGradients computes the gradients of W and b as defined by differentiating
  % the cost function with respects to the node in the computational graph and traversing it
  % likeso, until we reach the parameter variable of interest. As we have a 2-layer network
  % this results in us needing to update DL/DW1, DL/DW2, DL/Db1 and DL/Db2.
  % INPUT:
  %   X -- The current data batch of size (d, N_batch)
  %   Y -- The current one-hot label representation of size (K, N_batch) (true distribution)
  %   P -- The current probability distribution calculated by our parametrized model.
  %   W -- The current weight cell of size (2, 1) containing W1 and W2
  %   b -- The current bias cell of size (2,1) containing b1 and b2
  %   N -- The current batch size, i.e. N == N_batch
  %   lambda -- The current scalar regularization parameter
  %
  % OUTPUT:
  %   grad_W -- The gradient w.r.t W of size (K, d)
  %   grad_b -- The gradient w.r.t b of size (K, 1)
  % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
  layers = length(W);
  grad_W = cell(layers,1);
  grad_b = cell(layers,1);
  grad_W = cellfun(@(x, y) double(zeros(size(y))), grad_W, W, 'UniformOutput', false);
  grad_b = cellfun(@(x, y) double(zeros(size(y))), grad_b, b, 'UniformOutput', false);
  g = cell(N, 1);
  for i=1:N
    Yi = Y(:,i);
    Pi = P(:,i);
    g{i} = -(Yi-Pi)'; % We missed a f****** minus sign.
    hi = H{layers-1,1}(:,i);
    si = Shat{layers-1,1}(:,i);
    grad_b{layers,1} += g{i}';
    grad_W{layers,1} += g{i}'*hi';
    g{i} = g{i}*W{layers, 1};
    g{i} = g{i}*diag(si > 0);
  end
  %grad_W{layers, 1} = grad_W{layers, 1}/N + 2*lambda*W{layers, 1};
  %grad_b{layers, 1} = grad_b{layers,1}/N;
  for layer = layers-1:-1:1
    g = BatchNormBackPass(g, S{layer}, mus{layer}, vs{layer}, N);
    if (layer == 1)
      h = X;
    else
      h = H{layer-1,1};
    endif
    for i = 1:N
      grad_b{layer, 1} += g{i}';
      grad_W{layer, 1} += g{i}'*h(:,i)';
      if (layer != 1)%+ 2*lambda*W{layer,1};
        g{i} = g{i}*W{layer, 1};
        si = Shat{layer-1,1}(:,i);
        g{i} = g{i}*diag(si > 0);
      endif
    endfor
  endfor
  grad_b = cellfun(@(x) x/N, grad_b, 'UniformOutput', false);
  grad_W = cellfun(@(x, y) x/N + 2*lambda*y, grad_W, W, 'UniformOutput', false);

endfunction

function mu = ComputeMean(s)
  % ComputeAccuracy computes the proportion of correct guessed classes
  % of total classes tried. It sums the list of instances where y - facit
  % is equal to our guessed class, extracted as the max prob-value from our
  % distribution vector P, generated by evaluating the current parameters W and b.
  %
  % INPUT:
  %   s -- The current data batch of size (d, N_batch)
  %
  % OUTPUT:
  %   mu -- The scalar accuracy as calculated by the fraction of correct/total.
  % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
  mu = mean(s, 2);
endfunction

function v = ComputeVariance(s, mu)
  % ComputeAccuracy computes the proportion of correct guessed classes
  % of total classes tried. It sums the list of instances where y - facit
  % is equal to our guessed class, extracted as the max prob-value from our
  % distribution vector P, generated by evaluating the current parameters W and b.
  %
  % INPUT:
  %   s -- The current data batch of size (d, N_batch)
  %
  % OUTPUT:
  %   mu -- The scalar accuracy as calculated by the fraction of correct/total.
  % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
  n = size(s, 2);
  v = var(s, 0, 2);
  v = v*(n-1)/n;
endfunction

function [P, S, Shat, H, mus, vs] = EvaluateClassifier(X, W, b, varargin)
  % EvaluateClassifier computes the two layers of our forward pass. It is described
  % classicaly by an affine transformation that consists of a linear map and bias term.
  % W*X "twists and turns" the data, whereas the bias term translates it.
  % We then add our preferedd non-linearity, this time defined by the Relu model.
  % The relu model has non-saturating gradients and helps with disentanglement, representation
  % linear separability and "bagging". See 'Deep Sparse Rectifier Neural Networks' for more
  % information.
  % As our job is to find 1 of K classes that our image belongs to, our output
  % needs to reflect, in someway, or confidence in the different classes.
  % The softmax function produces a probability distribution over K classes
  % in the sense that each value is confined in [0,1] and sum(P) == 1. As it contains
  % exponentiations and normalizations, we conclude that our affine transformation
  % produces un-normalized (normalized by the division) log-probabilities (converted with exp).
  % This also explains the choice of cross-entropy loss, as that loss function minimizes
  % distributional distances between our guessed distribution from this function, and our
  % true labels in Y or y.
  % INPUT:
  %   X -- The current data batch of size (d, N_batch)
  %   W -- The current weight cell of size (2, 1) containing W1 and W2
  %   b -- The current bias cell of size (2, 1) containing b1 and b2
  %
  % OUTPUT:
  %   P -- the probability matrix for the classes of X of size (K, N)
  % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
  isTrain = length(varargin) == 0;
  layers = length(W);
  parameters = layers*2;
  S = cell(layers, 1);
  Shat = cell(layers-1, 1);
  H = cell(layers-1, 1);
  if (isTrain)
    mus = cell(layers, 1);
    vs = cell(layers, 1);
  else
    mus = varargin{1,1};
    vs = varargin{1,2};
  endif

  for layer = 1:(layers-1)
    s = W{layer, 1}*X + b{layer, 1};
    S(layer, 1) = s;
    if (isTrain)
      mus(layer, 1) = ComputeMean(s);
      vs(layer, 1) = ComputeVariance(s, mus{layer,1});
    endif
    s = BatchNormalize(s, mus{layer,1}, vs{layer,1});
    Shat{layer, 1} = s;
    h = Relu(s);
    H(layer, 1) = h;
    X = h;
  endfor
  s = W{layers, 1}*X + b{layers, 1};
  S{layers, 1} = s;
  P = Softmax(s);
endfunction

function params = Generateparams(min_value, max_value, no_params)
  % Generateparams uniformly at random samples on the log scale given by the input.
  % it generates a variable amount of results and returns a vector containing these.
  %   min_value -- the lower log-bound of sampling
  %   max_value -- the upper log-bound of sampling
  %   no_params -- The number of parameters to sample
  %
  % OUTPUT:
  %   params -- A populated parameter vector of size (no_params, 1)
  % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
params = 10.^(min_value+(max_value-min_value)*rand(no_params,1)');
endfunction

function maxDiff = GradChecker(grad_analytic, grad_numeric)
  % Self implemented gradient checker. It follows the instruction from the lab
  % specification and computes the relative error between our gradient and the
  % numerically generated one.
  % INPUT:
  %   grad_analytic -- our calculated gradient of size (K, d)
  %   grad_numeric  -- Some numerically calculated gradient approximation (K, d)
  %
  % OUTPUT:
  %   maxDiff -- the largest recorded element-wise difference between our gradients.
  % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
  difference = norm(grad_analytic - grad_numeric)./max(1e-6, norm(grad_analytic)+norm(grad_numeric));
  maxDiff = max(max(difference));
  return;
endfunction

function [W, b] = Initialize(d, layerData, initType)
  % Initialize creates our W and b cells and populates them with random values.
  % It either utilizes a random gaussian prior on the parameters or a Xavier prior.
  % The role of the Xavier prior is to keep X and and W*X equivariant, to increase
  % stability of the model and increase speed in our iterations.
  % INPUT:
  %   d -- The dimensionality of our input X.
  %   layerData -- A vector containing the number of nodes in each layer, including last.
  %   initType -- Optional variable that can invoke xavier-initialization.
  %
  % OUTPUT:
  %   W -- A populated weight cell array of size (layers, 1)
  %   b -- A populated bias cell array of size (layers, 1)
  % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
  mu = 0;
  variance = 0.000001;
  layers = length(layerData);
  W = cell(layers,1);
  b = cell(layers,1);

  W(1, 1) = double(randn(layerData(1), d));
  b(1, 1) = double(zeros(layerData(1), 1));
  for layer = 2:layers
    W(layer, 1) = double(randn(layerData(layer), layerData(layer-1)));
    b(layer, 1) = double(zeros(layerData(layer), 1));
  endfor
  if (initType == 'xavier')
    W = cellfun(@(x) x*sqrt(1/length(x)) + mu, W, 'UniformOutput', false);
    return;
  end
  W = cellfun(@(x) x*sqrt(variance) + mu, W, 'UniformOutput', false);
endfunction

function [Wm, bm] = InitializeMomentum(W, b)
  % Initialize creates our Wm and bm cells and populates them with zeros.
  % INPUT:
  %   W -- the current weight cell of size (2,1) containing W1 and W2
  %   b -- the current bias cell of size (2,1) containing b1 and b2
  %
  % OUTPUT:
  %   Wm -- A populated weight momentum cell of size (2, 1)
  %   bm -- A populated bias momentum cell of size (2, 1)
  % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
  layers = length(W);
  Wm = cell(layers,1);
  bm = cell(layers,1);
  for layer = 1:layers
    Wm(layer, 1) = zeros(size(W{layer, 1}));
    bm(layer, 1) = zeros(size(b{layer, 1}));
  endfor

endfunction


function [X, Y, y, N] = LoadAll(fileList)
  X = [];
  Y = [];
  y = [];
  for i=1:length(fileList)
  [Xi, Yi, yi, N] = LoadBatch(fileList{i});
  X = [X Xi];
  Y = [Y Yi];
  y = [y yi'];
  endfor
  N = columns(X);
endfunction

function [X, Y, y, N] = LoadBatch(fileName)
  % LoadBatch loads a batch of data as defined in the lab instructions.
  % It reads from that loaded file and puts the data in our X variable
  % our labels in the y variable and then additionally creates a one-hot
  % representation of y and saves it to Y.
  % INPUT:
  %   fileName -- a string containing a .mat-file in the path-environment.
  %
  % OUTPUT:
  %   X -- Matrix of data with size (d, N)
  %   Y -- Matrix of one-hot repesentation with size (K, N)
  %   y -- Vector of ground-truth labels of size (1, N)
  %   N -- Scalar value with value N. For readibility in subs
  % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
  warning('off','all');
  addpath Datasets/cifar-10-batches-mat/;
  inputFile = load(fileName);
  X = im2double(inputFile.data)';
  y = double(inputFile.labels) +1;
  Y = y == 1:max(y);
  N = columns(X);
  Y = Y';
endfunction

function [Wstar, bstar, Wm, bm, mav, vav] = MiniBatchGD(Xbatch, Ybatch,eta, W, b, Wm, bm, N,lambda, rho, mav, vav, alph)
  % MiniBatchGD performs the entire forward and backward pass.
  % It divides the dataset (X, Y)-pairs into batches and computes
  % the forward pass to generate a batch-determined class probability distribution
  % P. It then computes the backwardpass gradients and updates the W and b
  % elements, scaled by our learning-rate eta. As W and b isn't dependent on
  % the number of training-examples (N) (check their shapes), they are always
  % full.
  % INPUT:
  %   X -- The current data batch of size (d, N)
  %   Y -- The current one-hot label matrix of size (K, N)
  %   eta -- the current learning rate
  %   W -- The current weight cell of size (2, 1) containing W1 and W2
  %   b -- The current bias cell of size (2, 1) containing b1 and b2
  %   Wm -- The current weight momentum cell of size (2, 1) containing Wm1 and Wm2
  %   bm -- The current bias momentum cell of size (2, 1) containing bm1 and bm2
  %   N -- The scalar value representing number of examples.
  %   lambda -- The weighting parameter of our regularization term.
  %   rho -- The momentum term, a scalar betwenn 0 and 1
  %
  % OUTPUT:
  %   Wstar -- The final computed gradient for W of size (K, d)
  %   bstar -- The final computed gradient for b of size (K, 1)
  %   Wm -- The updated weight momentum cell of size (2, 1) containing Wm1 and Wm2
  %   bm -- The updated bias momentum cell of size (2, 1) containing bm1 and bm2
  % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
  layers = length(W);
  [P, S, Shat, H, mus, vs] = EvaluateClassifier(Xbatch, W, b);
  if (size(mav{1,1}, 1) == 0)
    mav = mus;
    vav = vs;
  else
    mav = cellfun(@(x, y) alph*x + (1-alph)*y, mav, mus, 'UniformOutput', false);
    vav = cellfun(@(x, y) alph*x + (1-alph)*y, vav, vs, 'UniformOutput', false);
  endif

  [grad_b, grad_W] = ComputeGradients(Xbatch, Ybatch, P, S, Shat, H, mus, vs, W, b, N, lambda);
  Wm = cellfun(@(x, y) rho*x + eta*y, Wm, grad_W, 'UniformOutput', false);
  Wstar = cellfun(@(x, y) x-y, W, Wm, 'UniformOutput', false);
  bm = cellfun(@(x, y) rho*x + eta*y, bm, grad_b, 'UniformOutput', false);
  bstar = cellfun(@(x, y) x-y, b, bm, 'UniformOutput', false);
endfunction

function [Xstar, mean_of_X] = Preprocess(X)
  % Preprocess normalizes the dataset by subtracting the mean.
  % This will give the resulting data a zero mean across the examples.
  % INPUT:
  %   X -- An un-normalized data matrix of size (d,N).
  %
  % OUTPUT:
  %   Xstar -- a normalized data matrix of size (d,N)
  %   mean_of_x -- a mean-vector of size (d)
  % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
  mean_of_X = mean(X, 2);
  Xstar = X - repmat(mean_of_X, [1, size(X, 2)]);
endfunction

function h = Relu(s)
  % This activation function is the relu-activation function.
  % INPUT:
  %   s -- The result of an affine W*X+b-transformation of size (m, N)
  %
  % OUTPUT:
  %   h --  non-linearity output of size (m, N)
  % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
  h = max(0, s);
endfunction

function p = Softmax(s)
  % This softmax implementation follow the standard scheme for such a function.
  % It's range is constrained to [0,1] and by summing over the exponents in the
  % denominator, we ensure a 'proper' distribution in a Kolmogorovian sense.
  % The commented line is to increase numerical stability once the gradients are
  % correct.
  % INPUT:
  %   s -- The result of an affine W*X+b-transformation of size (K, N)
  %
  % OUTPUT:
  %   p -- the probability matrix for the classes of X of size (K, N)
  % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
  %s -= max(s);
  p = exp(s) ./ sum(exp(s));
endfunction

function [grad_b, grad_W] = ComputeGradsNumSlow(X, Y, W, b, N, lambda, h)

grad_W = cell(numel(W), 1);
grad_b = cell(numel(b), 1);

for j=1:length(b)
    grad_b{j} = zeros(size(b{j}));

    for i=1:length(b{j})

        b_try = b;
        b_try{j}(i) = b_try{j}(i) - h;
        c1 = ComputeCost(X, Y, W, b_try, N, lambda);

        b_try = b;
        b_try{j}(i) = b_try{j}(i) + h;
        c2 = ComputeCost(X, Y, W, b_try, N, lambda);

        grad_b{j}(i) = (c2-c1) / (2*h);
    end
end

for j=1:length(W)
    grad_W{j} = zeros(size(W{j}));

    for i=1:numel(W{j})

        W_try = W;
        W_try{j}(i) = W_try{j}(i) - h;
        c1 = ComputeCost(X, Y, W_try, b, N, lambda);

        W_try = W;
        W_try{j}(i) = W_try{j}(i) + h;
        c2 = ComputeCost(X, Y, W_try, b, N, lambda);

        grad_W{j}(i) = (c2-c1) / (2*h);
    end
end
endfunction



[Xtrain, Ytrain, ytrain, Ntrain] = LoadAll({'data_batch_1.mat'});
[Xval, Yval, yval, Nval] = LoadBatch('data_batch_5.mat');
[Xtest, Ytest, ytest, Ntest] = LoadBatch('test_batch.mat');
ytrain = ytrain';
[Xtrain, mean_of_Xtrain] = Preprocess(Xtrain);
d = rows(Xtrain);
K = rows(Ytrain);
layerData = [50, K];
[W, b] = Initialize(d, layerData, 'gaussi');
n_epochs = 1;
n_batch = 512;
no_etas = 6;
no_lambdas = 1;
etas = [0.05];
decayRate = 0.9;
lambdas = [1e-6];
titleText = ['searching over a total of ' num2str(no_etas*no_lambdas) ' parameters.'];
disp(titleText);
%lambda = 0.000056;
%eta = 0.017260;
alph = 0.99;
N = Ntrain;
rho = 0.999;
J_train = [];
J_val = [];
tic;
message = ['Initializing ' num2str(length(layerData)) '-layer training with ' num2str(N) ' examples and ' num2str(n_epochs) ' epochs...'];
disp(message);
bestAccuracies = [];
for eta = etas
  disp(eta);
  for lambda = lambdas
    [W, b] = Initialize(d, layerData, 'gaussi');
    mav = cell(size(layerData));
    vav = cell(size(layerData));
    J_train = [];
    J_val = [];
    tempAccuracies = [];
    tic;
    valCost = 10000;
    iterator = 0;
    for epoch = 1:n_epochs
      [Wm, bm] = InitializeMomentum(W, b);
      for j = 1:N/n_batch
        j_start = (j-1)*n_batch + 1;
        j_end = j*n_batch;
        inds = j_start:j_end;
        Xbatch = Xtrain(:,inds);
        Ybatch = Ytrain(:,inds);
        [W,b,Wm,bm,mav,vav] = MiniBatchGD(Xbatch, Ybatch, eta, W, b, Wm, bm, n_batch,lambda, rho, mav, vav, alph);
      endfor
      eta = eta*decayRate;
      costTrain = ComputeCost(Xtrain, Ytrain, W, b, N, lambda, mav, vav);
      accTrain = ComputeAccuracy(Xtrain, ytrain, W, b, N, mav, vav);
      if epoch == 1
        timeMessage = ['Estimated total run time in minutes: ' num2str(round(toc*n_epochs/60))];
        disp(timeMessage);
      endif
      costMessage = ['Cost at epoch ' num2str(epoch) ': ' num2str(costTrain)];
      disp(costMessage);
      accMessage = ['Accuracy at epoch ' num2str(epoch) ': ' num2str(accTrain)];
      disp(accMessage);
      J_train = [J_train costTrain];
      costVal = ComputeCost(Xval-repmat(mean_of_Xtrain,[1,size(Xval,2)]), Yval, W, b, Nval, lambda, mav, vav);
      costValMessage = ['Validation cost at epoch ' num2str(epoch) ': ' num2str(costVal)];
      accVal = ComputeAccuracy(Xval-repmat(mean_of_Xtrain,[1,size(Xval,2)]), yval, W, b, N, mav, vav);
      costAccMessage = ['Validation accuracy at epoch ' num2str(epoch) ': ' num2str(accVal)];
      disp(costValMessage);
      disp(costAccMessage);
      J_val = [J_val costVal];
    endfor
    tempAccuracies = [tempAccuracies ComputeAccuracy(Xtest-repmat(mean_of_Xtrain,[1,size(Xtest,2)]),ytest, W, b,Ntest, mav, vav)];
    bestAccuracies = [bestAccuracies ['accuracy: ' num2str(max(tempAccuracies)) ' for lambda ' num2str(lambda) ' and eta ' num2str(eta) char(10)]];
    graphics_toolkit gnuplot;
    finalAcc = ['Final model accuracy is: ' num2str(ComputeAccuracy(Xtest-repmat(mean_of_Xtrain,[1,size(Xtest,2)]), ytest, W, b, Ntest, mav, vav)*100) ' %'];
    disp(finalAcc);
    fig = figure();
    set(fig, 'visible', 'off');
    set(0, 'defaultaxesfontname', 'Helvetica');
    hold on;
    titleText = ['Cost with layers = ' num2str(length(layerData)) ' lambda = ' num2str(lambda) ' #epochs = ' num2str(n_epochs) ' #batches = ' num2str(n_batch) ' eta = ' num2str(eta) ' examples = ' num2str(N)];
    plot(1:n_epochs, J_train, 'b', 1:n_epochs, J_val, 'y');
    title(titleText);
    imageName = ['eta' num2str(eta) '+lambda' num2str(lambda) '.jpg'];
    legend('Training Cost', 'Validation Cost');
    print(imageName);
  endfor
endfor
save bestAccuracies.txt bestAccuracies;
